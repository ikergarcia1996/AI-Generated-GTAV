learning_rate: 1e-4
min_learning_rate: 1e-6
weight_decay: 0.01
batch_size: 32
validation_batch_size: 16
num_epochs: 3
gradient_accumulation_steps: 2
seed: 42
use_wandb: true
output_dir: "checkpoints"
kl_weight: 1e-6  # Weight for KL divergence loss
max_steps: -1  # -1 means no maximum steps limit
validation_steps: 10000
save_every: 10000
logging_steps: 4
warnup_ratio: 0.03