{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from model.dit import DiT_models\n",
    "from model.vae import VAE_models\n",
    "from torchvision.io import read_video, write_video\n",
    "from utils import  sigmoid_beta_schedule\n",
    "from tqdm import tqdm\n",
    "from einops import rearrange\n",
    "from torch import autocast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18840/3437294327.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(\"checkpoints/oasis500m.pt\")\n"
     ]
    }
   ],
   "source": [
    "assert torch.cuda.is_available()\n",
    "device = \"cuda:0\"\n",
    "ckpt = torch.load(\"checkpoints/oasis500m.pt\")\n",
    "model = DiT_models[\"DiT-S/2\"]()\n",
    "model.load_state_dict(ckpt, strict=False)\n",
    "model = model.to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DiT(\n",
       "  (x_embedder): PatchEmbed(\n",
       "    (proj): Conv2d(16, 1024, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (norm): Identity()\n",
       "  )\n",
       "  (t_embedder): TimestepEmbedder(\n",
       "    (mlp): Sequential(\n",
       "      (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "      (1): SiLU()\n",
       "      (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (spatial_rotary_emb): RotaryEmbedding()\n",
       "  (temporal_rotary_emb): RotaryEmbedding()\n",
       "  (external_cond): Linear(in_features=25, out_features=1024, bias=True)\n",
       "  (blocks): ModuleList(\n",
       "    (0-15): 16 x SpatioTemporalDiTBlock(\n",
       "      (s_norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=False)\n",
       "      (s_attn): SpatialAxialAttention(\n",
       "        (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "        (to_out): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "      )\n",
       "      (s_norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=False)\n",
       "      (s_mlp): Mlp(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (act): GELU(approximate='tanh')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (s_adaLN_modulation): Sequential(\n",
       "        (0): SiLU()\n",
       "        (1): Linear(in_features=1024, out_features=6144, bias=True)\n",
       "      )\n",
       "      (t_norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=False)\n",
       "      (t_attn): TemporalAxialAttention(\n",
       "        (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "        (to_out): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "      )\n",
       "      (t_norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=False)\n",
       "      (t_mlp): Mlp(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (act): GELU(approximate='tanh')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (t_adaLN_modulation): Sequential(\n",
       "        (0): SiLU()\n",
       "        (1): Linear(in_features=1024, out_features=6144, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (final_layer): FinalLayer(\n",
       "    (norm_final): LayerNorm((1024,), eps=1e-06, elementwise_affine=False)\n",
       "    (linear): Linear(in_features=1024, out_features=64, bias=True)\n",
       "    (adaLN_modulation): Sequential(\n",
       "      (0): SiLU()\n",
       "      (1): Linear(in_features=1024, out_features=2048, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18840/631049858.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  vae_ckpt = torch.load(\"checkpoints/vit-l-20.pt\")\n"
     ]
    }
   ],
   "source": [
    "vae_ckpt = torch.load(\"checkpoints/vit-l-20.pt\")\n",
    "vae = VAE_models[\"vit-l-20-shallow-encoder\"]()\n",
    "vae.load_state_dict(vae_ckpt)\n",
    "vae = vae.to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AutoencoderKL(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 1024, kernel_size=(20, 20), stride=(20, 20))\n",
       "    (norm): Identity()\n",
       "  )\n",
       "  (encoder): ModuleList(\n",
       "    (0-5): 6 x AttentionBlock(\n",
       "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (enc_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "  (quant_conv): Linear(in_features=1024, out_features=32, bias=True)\n",
       "  (post_quant_conv): Linear(in_features=16, out_features=1024, bias=True)\n",
       "  (decoder): ModuleList(\n",
       "    (0-11): 12 x AttentionBlock(\n",
       "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dec_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "  (predictor): Linear(in_features=1024, out_features=1200, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 1  # Batch size\n",
    "total_frames = 32  # Number of frames to generate\n",
    "max_noise_level = 1000  # Maximum noise level for diffusion\n",
    "ddim_noise_steps = 16  # Number of denoising steps\n",
    "noise_range = torch.linspace(\n",
    "    -1, max_noise_level - 1, ddim_noise_steps + 1\n",
    ")  # Noise range\n",
    "noise_abs_max = 20  # Maximum absolute value for noise\n",
    "ctx_max_noise_idx = ddim_noise_steps // 10 * 3  # Context maximum noise index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ -1.0000,  61.5000, 124.0000, 186.5000, 249.0000, 311.5000, 374.0000,\n",
       "        436.5000, 499.0000, 561.5000, 624.0000, 686.5000, 749.0000, 811.5000,\n",
       "        874.0000, 936.5000, 999.0000])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noise_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctx_max_noise_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_id = \"snippy-chartreuse-mastiff-f79998db196d-20220401-224517.chunk_001\"\n",
    "mp4_path = f\"sample_data/{video_id}.mp4\"\n",
    "actions_path = f\"sample_data/{video_id}.actions.pt\"\n",
    "video = (\n",
    "    read_video(mp4_path, pts_unit=\"sec\")[0].float() / 255\n",
    ")  # Normalize pixel values to [0, 1] range\n",
    "#actions = one_hot_actions(torch.load(actions_path))\n",
    "offset = 100\n",
    "video = video[offset : offset + total_frames].unsqueeze(\n",
    "    0\n",
    ")  # Selects a specific segment using offset\n",
    "#actions = actions[offset : offset + total_frames].unsqueeze(\n",
    "#    0\n",
    "#)  # Selects a specific segment using offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 360, 640, 3])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#actions.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 360, 640, 3])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_prompt_frames = 1\n",
    "x = video[:, :n_prompt_frames]\n",
    "x = x.to(device)\n",
    "#actions = actions.to(device)\n",
    "\n",
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 360, 640])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 576, 16])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaling_factor = 0.07843137255\n",
    "x = rearrange(x, \"b t h w c -> (b t) c h w\")\n",
    "print(x.size())\n",
    "H, W = x.shape[-2:]\n",
    "with torch.no_grad():\n",
    "    x = vae.encode(x * 2 - 1).mean * scaling_factor\n",
    "\n",
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 16, 18, 32])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = rearrange(\n",
    "    x,\n",
    "    \"(b t) (h w) c -> b t c h w\",\n",
    "    t=n_prompt_frames,\n",
    "    h=H // vae.patch_size,\n",
    "    w=W // vae.patch_size,\n",
    ")\n",
    "\n",
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000])\n",
      "torch.Size([1000])\n",
      "torch.Size([1000])\n",
      "torch.Size([1000, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "betas = sigmoid_beta_schedule(max_noise_level).to(device) \n",
    "print(betas.size())\n",
    "alphas = 1.0 - betas\n",
    "print(alphas.size())\n",
    "alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
    "print(alphas_cumprod.size())\n",
    "alphas_cumprod = rearrange(alphas_cumprod, \"T -> T 1 1 1\")\n",
    "print(alphas_cumprod.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = n_prompt_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 16, 18, 32])\n",
      "torch.Size([1, 1, 16, 18, 32])\n",
      "torch.Size([1, 2, 16, 18, 32])\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "chunk = torch.randn((B, 1, *x.shape[-3:]), device=device)\n",
    "chunk = torch.clamp(chunk, -noise_abs_max, +noise_abs_max)\n",
    "print(chunk.size())\n",
    "print(x.size())\n",
    "x = torch.cat([x, chunk], dim=1)\n",
    "print(x.size())\n",
    "start_frame = max(0, i + 1 - model.max_frames)\n",
    "print(start_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min(16, 3) = 3\n",
      "ctx_noise_idx 3\n",
      "t_ctx torch.Size([1, 1]) tensor([[186]], device='cuda:0')\n",
      "t torch.Size([1, 1]) tensor([[999]], device='cuda:0')\n",
      "torch.Size([1, 1]) tensor([[936]], device='cuda:0')\n",
      "torch.Size([1, 2]) tensor([[186, 999]], device='cuda:0')\n",
      "torch.Size([1, 2]) tensor([[186, 936]], device='cuda:0')\n",
      "torch.Size([1, 2, 16, 18, 32])\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1, 2])\n",
      "t_next tensor([[186, 936]], device='cuda:0')\n",
      "torch.Size([1, 1, 16, 18, 32])\n",
      "torch.Size([1, 1, 16, 18, 32])\n",
      "torch.Size([1, 2, 16, 18, 32])\n",
      "torch.Size([1, 2, 16, 18, 32])\n",
      "torch.Size([1, 2, 16, 18, 32])\n",
      "\n",
      "\n",
      "min(15, 3) = 3\n",
      "ctx_noise_idx 3\n",
      "t_ctx torch.Size([1, 1]) tensor([[186]], device='cuda:0')\n",
      "t torch.Size([1, 1]) tensor([[936]], device='cuda:0')\n",
      "torch.Size([1, 1]) tensor([[874]], device='cuda:0')\n",
      "torch.Size([1, 2]) tensor([[186, 936]], device='cuda:0')\n",
      "torch.Size([1, 2]) tensor([[186, 874]], device='cuda:0')\n",
      "torch.Size([1, 2, 16, 18, 32])\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1, 2])\n",
      "t_next tensor([[186, 874]], device='cuda:0')\n",
      "torch.Size([1, 1, 16, 18, 32])\n",
      "torch.Size([1, 1, 16, 18, 32])\n",
      "torch.Size([1, 2, 16, 18, 32])\n",
      "torch.Size([1, 2, 16, 18, 32])\n",
      "torch.Size([1, 2, 16, 18, 32])\n",
      "\n",
      "\n",
      "min(14, 3) = 3\n",
      "ctx_noise_idx 3\n",
      "t_ctx torch.Size([1, 1]) tensor([[186]], device='cuda:0')\n",
      "t torch.Size([1, 1]) tensor([[874]], device='cuda:0')\n",
      "torch.Size([1, 1]) tensor([[811]], device='cuda:0')\n",
      "torch.Size([1, 2]) tensor([[186, 874]], device='cuda:0')\n",
      "torch.Size([1, 2]) tensor([[186, 811]], device='cuda:0')\n",
      "torch.Size([1, 2, 16, 18, 32])\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1, 2])\n",
      "t_next tensor([[186, 811]], device='cuda:0')\n",
      "torch.Size([1, 1, 16, 18, 32])\n",
      "torch.Size([1, 1, 16, 18, 32])\n",
      "torch.Size([1, 2, 16, 18, 32])\n",
      "torch.Size([1, 2, 16, 18, 32])\n",
      "torch.Size([1, 2, 16, 18, 32])\n",
      "\n",
      "\n",
      "min(13, 3) = 3\n",
      "ctx_noise_idx 3\n",
      "t_ctx torch.Size([1, 1]) tensor([[186]], device='cuda:0')\n",
      "t torch.Size([1, 1]) tensor([[811]], device='cuda:0')\n",
      "torch.Size([1, 1]) tensor([[749]], device='cuda:0')\n",
      "torch.Size([1, 2]) tensor([[186, 811]], device='cuda:0')\n",
      "torch.Size([1, 2]) tensor([[186, 749]], device='cuda:0')\n",
      "torch.Size([1, 2, 16, 18, 32])\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1, 2])\n",
      "t_next tensor([[186, 749]], device='cuda:0')\n",
      "torch.Size([1, 1, 16, 18, 32])\n",
      "torch.Size([1, 1, 16, 18, 32])\n",
      "torch.Size([1, 2, 16, 18, 32])\n",
      "torch.Size([1, 2, 16, 18, 32])\n",
      "torch.Size([1, 2, 16, 18, 32])\n",
      "\n",
      "\n",
      "min(12, 3) = 3\n",
      "ctx_noise_idx 3\n",
      "t_ctx torch.Size([1, 1]) tensor([[186]], device='cuda:0')\n",
      "t torch.Size([1, 1]) tensor([[749]], device='cuda:0')\n",
      "torch.Size([1, 1]) tensor([[686]], device='cuda:0')\n",
      "torch.Size([1, 2]) tensor([[186, 749]], device='cuda:0')\n",
      "torch.Size([1, 2]) tensor([[186, 686]], device='cuda:0')\n",
      "torch.Size([1, 2, 16, 18, 32])\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1, 2])\n",
      "t_next tensor([[186, 686]], device='cuda:0')\n",
      "torch.Size([1, 1, 16, 18, 32])\n",
      "torch.Size([1, 1, 16, 18, 32])\n",
      "torch.Size([1, 2, 16, 18, 32])\n",
      "torch.Size([1, 2, 16, 18, 32])\n",
      "torch.Size([1, 2, 16, 18, 32])\n",
      "\n",
      "\n",
      "min(11, 3) = 3\n",
      "ctx_noise_idx 3\n",
      "t_ctx torch.Size([1, 1]) tensor([[186]], device='cuda:0')\n",
      "t torch.Size([1, 1]) tensor([[686]], device='cuda:0')\n",
      "torch.Size([1, 1]) tensor([[624]], device='cuda:0')\n",
      "torch.Size([1, 2]) tensor([[186, 686]], device='cuda:0')\n",
      "torch.Size([1, 2]) tensor([[186, 624]], device='cuda:0')\n",
      "torch.Size([1, 2, 16, 18, 32])\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1, 2])\n",
      "t_next tensor([[186, 624]], device='cuda:0')\n",
      "torch.Size([1, 1, 16, 18, 32])\n",
      "torch.Size([1, 1, 16, 18, 32])\n",
      "torch.Size([1, 2, 16, 18, 32])\n",
      "torch.Size([1, 2, 16, 18, 32])\n",
      "torch.Size([1, 2, 16, 18, 32])\n",
      "\n",
      "\n",
      "min(10, 3) = 3\n",
      "ctx_noise_idx 3\n",
      "t_ctx torch.Size([1, 1]) tensor([[186]], device='cuda:0')\n",
      "t torch.Size([1, 1]) tensor([[624]], device='cuda:0')\n",
      "torch.Size([1, 1]) tensor([[561]], device='cuda:0')\n",
      "torch.Size([1, 2]) tensor([[186, 624]], device='cuda:0')\n",
      "torch.Size([1, 2]) tensor([[186, 561]], device='cuda:0')\n",
      "torch.Size([1, 2, 16, 18, 32])\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1, 2])\n",
      "t_next tensor([[186, 561]], device='cuda:0')\n",
      "torch.Size([1, 1, 16, 18, 32])\n",
      "torch.Size([1, 1, 16, 18, 32])\n",
      "torch.Size([1, 2, 16, 18, 32])\n",
      "torch.Size([1, 2, 16, 18, 32])\n",
      "torch.Size([1, 2, 16, 18, 32])\n",
      "\n",
      "\n",
      "min(9, 3) = 3\n",
      "ctx_noise_idx 3\n",
      "t_ctx torch.Size([1, 1]) tensor([[186]], device='cuda:0')\n",
      "t torch.Size([1, 1]) tensor([[561]], device='cuda:0')\n",
      "torch.Size([1, 1]) tensor([[499]], device='cuda:0')\n",
      "torch.Size([1, 2]) tensor([[186, 561]], device='cuda:0')\n",
      "torch.Size([1, 2]) tensor([[186, 499]], device='cuda:0')\n",
      "torch.Size([1, 2, 16, 18, 32])\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1, 2])\n",
      "t_next tensor([[186, 499]], device='cuda:0')\n",
      "torch.Size([1, 1, 16, 18, 32])\n",
      "torch.Size([1, 1, 16, 18, 32])\n",
      "torch.Size([1, 2, 16, 18, 32])\n",
      "torch.Size([1, 2, 16, 18, 32])\n",
      "torch.Size([1, 2, 16, 18, 32])\n",
      "\n",
      "\n",
      "min(8, 3) = 3\n",
      "ctx_noise_idx 3\n",
      "t_ctx torch.Size([1, 1]) tensor([[186]], device='cuda:0')\n",
      "t torch.Size([1, 1]) tensor([[499]], device='cuda:0')\n",
      "torch.Size([1, 1]) tensor([[436]], device='cuda:0')\n",
      "torch.Size([1, 2]) tensor([[186, 499]], device='cuda:0')\n",
      "torch.Size([1, 2]) tensor([[186, 436]], device='cuda:0')\n",
      "torch.Size([1, 2, 16, 18, 32])\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1, 2])\n",
      "t_next tensor([[186, 436]], device='cuda:0')\n",
      "torch.Size([1, 1, 16, 18, 32])\n",
      "torch.Size([1, 1, 16, 18, 32])\n",
      "torch.Size([1, 2, 16, 18, 32])\n",
      "torch.Size([1, 2, 16, 18, 32])\n",
      "torch.Size([1, 2, 16, 18, 32])\n",
      "\n",
      "\n",
      "min(7, 3) = 3\n",
      "ctx_noise_idx 3\n",
      "t_ctx torch.Size([1, 1]) tensor([[186]], device='cuda:0')\n",
      "t torch.Size([1, 1]) tensor([[436]], device='cuda:0')\n",
      "torch.Size([1, 1]) tensor([[374]], device='cuda:0')\n",
      "torch.Size([1, 2]) tensor([[186, 436]], device='cuda:0')\n",
      "torch.Size([1, 2]) tensor([[186, 374]], device='cuda:0')\n",
      "torch.Size([1, 2, 16, 18, 32])\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1, 2])\n",
      "t_next tensor([[186, 374]], device='cuda:0')\n",
      "torch.Size([1, 1, 16, 18, 32])\n",
      "torch.Size([1, 1, 16, 18, 32])\n",
      "torch.Size([1, 2, 16, 18, 32])\n",
      "torch.Size([1, 2, 16, 18, 32])\n",
      "torch.Size([1, 2, 16, 18, 32])\n",
      "\n",
      "\n",
      "min(6, 3) = 3\n",
      "ctx_noise_idx 3\n",
      "t_ctx torch.Size([1, 1]) tensor([[186]], device='cuda:0')\n",
      "t torch.Size([1, 1]) tensor([[374]], device='cuda:0')\n",
      "torch.Size([1, 1]) tensor([[311]], device='cuda:0')\n",
      "torch.Size([1, 2]) tensor([[186, 374]], device='cuda:0')\n",
      "torch.Size([1, 2]) tensor([[186, 311]], device='cuda:0')\n",
      "torch.Size([1, 2, 16, 18, 32])\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1, 2])\n",
      "t_next tensor([[186, 311]], device='cuda:0')\n",
      "torch.Size([1, 1, 16, 18, 32])\n",
      "torch.Size([1, 1, 16, 18, 32])\n",
      "torch.Size([1, 2, 16, 18, 32])\n",
      "torch.Size([1, 2, 16, 18, 32])\n",
      "torch.Size([1, 2, 16, 18, 32])\n",
      "\n",
      "\n",
      "min(5, 3) = 3\n",
      "ctx_noise_idx 3\n",
      "t_ctx torch.Size([1, 1]) tensor([[186]], device='cuda:0')\n",
      "t torch.Size([1, 1]) tensor([[311]], device='cuda:0')\n",
      "torch.Size([1, 1]) tensor([[249]], device='cuda:0')\n",
      "torch.Size([1, 2]) tensor([[186, 311]], device='cuda:0')\n",
      "torch.Size([1, 2]) tensor([[186, 249]], device='cuda:0')\n",
      "torch.Size([1, 2, 16, 18, 32])\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1, 2])\n",
      "t_next tensor([[186, 249]], device='cuda:0')\n",
      "torch.Size([1, 1, 16, 18, 32])\n",
      "torch.Size([1, 1, 16, 18, 32])\n",
      "torch.Size([1, 2, 16, 18, 32])\n",
      "torch.Size([1, 2, 16, 18, 32])\n",
      "torch.Size([1, 2, 16, 18, 32])\n",
      "\n",
      "\n",
      "min(4, 3) = 3\n",
      "ctx_noise_idx 3\n",
      "t_ctx torch.Size([1, 1]) tensor([[186]], device='cuda:0')\n",
      "t torch.Size([1, 1]) tensor([[249]], device='cuda:0')\n",
      "torch.Size([1, 1]) tensor([[186]], device='cuda:0')\n",
      "torch.Size([1, 2]) tensor([[186, 249]], device='cuda:0')\n",
      "torch.Size([1, 2]) tensor([[186, 186]], device='cuda:0')\n",
      "torch.Size([1, 2, 16, 18, 32])\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1, 2])\n",
      "t_next tensor([[186, 186]], device='cuda:0')\n",
      "torch.Size([1, 1, 16, 18, 32])\n",
      "torch.Size([1, 1, 16, 18, 32])\n",
      "torch.Size([1, 2, 16, 18, 32])\n",
      "torch.Size([1, 2, 16, 18, 32])\n",
      "torch.Size([1, 2, 16, 18, 32])\n",
      "\n",
      "\n",
      "min(3, 3) = 3\n",
      "ctx_noise_idx 3\n",
      "t_ctx torch.Size([1, 1]) tensor([[186]], device='cuda:0')\n",
      "t torch.Size([1, 1]) tensor([[186]], device='cuda:0')\n",
      "torch.Size([1, 1]) tensor([[124]], device='cuda:0')\n",
      "torch.Size([1, 2]) tensor([[186, 186]], device='cuda:0')\n",
      "torch.Size([1, 2]) tensor([[186, 124]], device='cuda:0')\n",
      "torch.Size([1, 2, 16, 18, 32])\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1, 2])\n",
      "t_next tensor([[186, 124]], device='cuda:0')\n",
      "torch.Size([1, 1, 16, 18, 32])\n",
      "torch.Size([1, 1, 16, 18, 32])\n",
      "torch.Size([1, 2, 16, 18, 32])\n",
      "torch.Size([1, 2, 16, 18, 32])\n",
      "torch.Size([1, 2, 16, 18, 32])\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for noise_idx in reversed(range(1, ddim_noise_steps + 1)):\n",
    "    ctx_noise_idx = min(noise_idx, ctx_max_noise_idx)\n",
    "    print(f\"min({noise_idx}, {ctx_max_noise_idx}) = {ctx_noise_idx}\")\n",
    "    print(\"ctx_noise_idx\", ctx_noise_idx)\n",
    "    \n",
    "    t_ctx = torch.full(\n",
    "                (B, i), noise_range[ctx_noise_idx], dtype=torch.long, device=device\n",
    "            )\n",
    "    print(\"t_ctx\", t_ctx.size(),t_ctx)\n",
    "    t = torch.full(\n",
    "                (B, 1), noise_range[noise_idx], dtype=torch.long, device=device\n",
    "            )\n",
    "    print(\"t\", t.size(),t)\n",
    "    t_next = torch.full(\n",
    "                (B, 1), noise_range[noise_idx - 1], dtype=torch.long, device=device\n",
    "            )\n",
    "    print(t_next.size(),t_next)\n",
    "    \n",
    "    t_next = torch.where(t_next < 0, t, t_next)\n",
    "    t = torch.cat([t_ctx, t], dim=1)\n",
    "    t_next = torch.cat([t_ctx, t_next], dim=1)\n",
    "    print(t.size(),t)\n",
    "    print(t_next.size(),t_next)\n",
    "\n",
    "    # sliding window\n",
    "    x_curr = x.clone()\n",
    "    x_curr = x_curr[:, start_frame:]\n",
    "    t = t[:, start_frame:]\n",
    "    t_next = t_next[:, start_frame:]\n",
    "    print(x_curr.size())\n",
    "    print(t.size())\n",
    "    print(t_next.size())\n",
    "    print(\"t_next\", t_next)\n",
    "\n",
    "    # add some noise to the context\n",
    "    ctx_noise = torch.randn_like(x_curr[:, :-1])\n",
    "    ctx_noise = torch.clamp(ctx_noise, -noise_abs_max, +noise_abs_max)\n",
    "    print(ctx_noise.size())\n",
    "    noise = alphas_cumprod[t[:, :-1]].sqrt() * x_curr[:, :-1] + (1 - alphas_cumprod[t[:, :-1]]).sqrt() * ctx_noise\n",
    "    print(noise.size())\n",
    "    x_curr[:, :-1] = noise\n",
    "\n",
    "    # Use DiT model to predict and remove noise\n",
    "    with torch.no_grad():\n",
    "        with autocast(\"cuda\", dtype=torch.half):\n",
    "            v = model(x_curr, t)#, actions[:, start_frame : i + 1])\n",
    "    print(v.size())\n",
    "\n",
    "    x_start = (\n",
    "                alphas_cumprod[t].sqrt() * x_curr - (1 - alphas_cumprod[t]).sqrt() * v\n",
    "            )\n",
    "    x_noise = ((1 / alphas_cumprod[t]).sqrt() * x_curr - x_start) / (\n",
    "        1 / alphas_cumprod[t] - 1\n",
    "    ).sqrt()\n",
    "\n",
    "    # get frame prediction\n",
    "    x_pred = (\n",
    "        alphas_cumprod[t_next].sqrt() * x_start\n",
    "        + x_noise * (1 - alphas_cumprod[t_next]).sqrt()\n",
    "    )\n",
    "    # Replace the generated frame in the input tensor\n",
    "    print(x.size())\n",
    "    x[:, -1:] = x_pred[:, -1:]\n",
    "    print(x.size())\n",
    "    print()\n",
    "    print()\n",
    "\n",
    "    \n",
    "    if noise_idx == 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "EinopsError",
     "evalue": " Error while processing rearrange-reduction pattern \"(b t) c h w -> b t h w c\".\n Input tensor shape: torch.Size([2, 3, 360, 640]). Additional info: {'t': 32}.\n Shape mismatch, can't divide axis of length 2 in chunks of 32",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEinopsError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/oasis/lib/python3.12/site-packages/einops/einops.py:523\u001b[0m, in \u001b[0;36mreduce\u001b[0;34m(tensor, pattern, reduction, **axes_lengths)\u001b[0m\n\u001b[1;32m    522\u001b[0m     recipe \u001b[38;5;241m=\u001b[39m _prepare_transformation_recipe(pattern, reduction, axes_names\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mtuple\u001b[39m(axes_lengths), ndim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(shape))\n\u001b[0;32m--> 523\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_apply_recipe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecipe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxes_lengths\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhashable_axes_lengths\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EinopsError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/envs/oasis/lib/python3.12/site-packages/einops/einops.py:234\u001b[0m, in \u001b[0;36m_apply_recipe\u001b[0;34m(backend, recipe, tensor, reduction_type, axes_lengths)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 234\u001b[0m     init_shapes, axes_reordering, reduced_axes, added_axes, final_shapes, n_axes_w_added \u001b[38;5;241m=\u001b[39m \u001b[43m_reconstruct_from_shape\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrecipe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxes_lengths\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;66;03m# shape or one of passed axes lengths is not hashable (i.e. they are symbols)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/oasis/lib/python3.12/site-packages/einops/einops.py:187\u001b[0m, in \u001b[0;36m_reconstruct_from_shape_uncached\u001b[0;34m(self, shape, axes_dims)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(length, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(known_product, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m length \u001b[38;5;241m%\u001b[39m known_product \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m EinopsError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape mismatch, can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt divide axis of length \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlength\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in chunks of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mknown_product\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    189\u001b[0m unknown_axis \u001b[38;5;241m=\u001b[39m unknown_axes[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mEinopsError\u001b[0m: Shape mismatch, can't divide axis of length 2 in chunks of 32",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mEinopsError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m      4\u001b[0m     x \u001b[38;5;241m=\u001b[39m (vae\u001b[38;5;241m.\u001b[39mdecode(x \u001b[38;5;241m/\u001b[39m scaling_factor) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m----> 5\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mrearrange\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m(b t) c h w -> b t h w c\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_frames\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# save video\u001b[39;00m\n\u001b[1;32m      8\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mclamp(x, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/oasis/lib/python3.12/site-packages/einops/einops.py:591\u001b[0m, in \u001b[0;36mrearrange\u001b[0;34m(tensor, pattern, **axes_lengths)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrearrange\u001b[39m(tensor: Union[Tensor, List[Tensor]], pattern: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39maxes_lengths) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    537\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;124;03m    einops.rearrange is a reader-friendly smart element reordering for multidimensional tensors.\u001b[39;00m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;124;03m    This operation includes functionality of transpose (axes permutation), reshape (view), squeeze, unsqueeze,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    589\u001b[0m \n\u001b[1;32m    590\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 591\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrearrange\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43maxes_lengths\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/oasis/lib/python3.12/site-packages/einops/einops.py:533\u001b[0m, in \u001b[0;36mreduce\u001b[0;34m(tensor, pattern, reduction, **axes_lengths)\u001b[0m\n\u001b[1;32m    531\u001b[0m     message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m Input is list. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    532\u001b[0m message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAdditional info: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(axes_lengths)\n\u001b[0;32m--> 533\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m EinopsError(message \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(e))\n",
      "\u001b[0;31mEinopsError\u001b[0m:  Error while processing rearrange-reduction pattern \"(b t) c h w -> b t h w c\".\n Input tensor shape: torch.Size([2, 3, 360, 640]). Additional info: {'t': 32}.\n Shape mismatch, can't divide axis of length 2 in chunks of 32"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oasis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
