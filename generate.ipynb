{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iker/miniconda3/envs/oasis/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from model.dit import DiT_models\n",
    "from model.vae import VAE_models\n",
    "from torchvision.io import read_video, write_video\n",
    "from utils import one_hot_actions, sigmoid_beta_schedule\n",
    "from tqdm import tqdm\n",
    "from einops import rearrange\n",
    "from torch import autocast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_35235/3437294327.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(\"checkpoints/oasis500m.pt\")\n"
     ]
    }
   ],
   "source": [
    "assert torch.cuda.is_available()\n",
    "device = \"cuda:0\"\n",
    "ckpt = torch.load(\"checkpoints/oasis500m.pt\")\n",
    "model = DiT_models[\"DiT-S/2\"]()\n",
    "model.load_state_dict(ckpt, strict=False)\n",
    "model = model.to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DiT(\n",
       "  (x_embedder): PatchEmbed(\n",
       "    (proj): Conv2d(16, 1024, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (norm): Identity()\n",
       "  )\n",
       "  (t_embedder): TimestepEmbedder(\n",
       "    (mlp): Sequential(\n",
       "      (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "      (1): SiLU()\n",
       "      (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (spatial_rotary_emb): RotaryEmbedding()\n",
       "  (temporal_rotary_emb): RotaryEmbedding()\n",
       "  (external_cond): Linear(in_features=25, out_features=1024, bias=True)\n",
       "  (blocks): ModuleList(\n",
       "    (0-15): 16 x SpatioTemporalDiTBlock(\n",
       "      (s_norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=False)\n",
       "      (s_attn): SpatialAxialAttention(\n",
       "        (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "        (to_out): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "      )\n",
       "      (s_norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=False)\n",
       "      (s_mlp): Mlp(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (act): GELU(approximate='tanh')\n",
       "        (drop1): Dropout(p=0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (drop2): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (s_adaLN_modulation): Sequential(\n",
       "        (0): SiLU()\n",
       "        (1): Linear(in_features=1024, out_features=6144, bias=True)\n",
       "      )\n",
       "      (t_norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=False)\n",
       "      (t_attn): TemporalAxialAttention(\n",
       "        (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "        (to_out): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "      )\n",
       "      (t_norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=False)\n",
       "      (t_mlp): Mlp(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (act): GELU(approximate='tanh')\n",
       "        (drop1): Dropout(p=0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (drop2): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (t_adaLN_modulation): Sequential(\n",
       "        (0): SiLU()\n",
       "        (1): Linear(in_features=1024, out_features=6144, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (final_layer): FinalLayer(\n",
       "    (norm_final): LayerNorm((1024,), eps=1e-06, elementwise_affine=False)\n",
       "    (linear): Linear(in_features=1024, out_features=64, bias=True)\n",
       "    (adaLN_modulation): Sequential(\n",
       "      (0): SiLU()\n",
       "      (1): Linear(in_features=1024, out_features=2048, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_35235/631049858.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  vae_ckpt = torch.load(\"checkpoints/vit-l-20.pt\")\n"
     ]
    }
   ],
   "source": [
    "vae_ckpt = torch.load(\"checkpoints/vit-l-20.pt\")\n",
    "vae = VAE_models[\"vit-l-20-shallow-encoder\"]()\n",
    "vae.load_state_dict(vae_ckpt)\n",
    "vae = vae.to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AutoencoderKL(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 1024, kernel_size=(20, 20), stride=(20, 20))\n",
       "    (norm): Identity()\n",
       "  )\n",
       "  (encoder): ModuleList(\n",
       "    (0-5): 6 x AttentionBlock(\n",
       "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (enc_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "  (quant_conv): Linear(in_features=1024, out_features=32, bias=True)\n",
       "  (post_quant_conv): Linear(in_features=16, out_features=1024, bias=True)\n",
       "  (decoder): ModuleList(\n",
       "    (0-11): 12 x AttentionBlock(\n",
       "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dec_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "  (predictor): Linear(in_features=1024, out_features=1200, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 1  # Batch size\n",
    "total_frames = 32  # Number of frames to generate\n",
    "max_noise_level = 1000  # Maximum noise level for diffusion\n",
    "ddim_noise_steps = 100  # Number of denoising steps\n",
    "noise_range = torch.linspace(\n",
    "    -1, max_noise_level - 1, ddim_noise_steps + 1\n",
    ")  # Noise range\n",
    "noise_abs_max = 20  # Maximum absolute value for noise\n",
    "ctx_max_noise_idx = ddim_noise_steps // 10 * 3  # Context maximum noise index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ -1.,   9.,  19.,  29.,  39.,  49.,  59.,  69.,  79.,  89.,  99., 109.,\n",
       "        119., 129., 139., 149., 159., 169., 179., 189., 199., 209., 219., 229.,\n",
       "        239., 249., 259., 269., 279., 289., 299., 309., 319., 329., 339., 349.,\n",
       "        359., 369., 379., 389., 399., 409., 419., 429., 439., 449., 459., 469.,\n",
       "        479., 489., 499., 509., 519., 529., 539., 549., 559., 569., 579., 589.,\n",
       "        599., 609., 619., 629., 639., 649., 659., 669., 679., 689., 699., 709.,\n",
       "        719., 729., 739., 749., 759., 769., 779., 789., 799., 809., 819., 829.,\n",
       "        839., 849., 859., 869., 879., 889., 899., 909., 919., 929., 939., 949.,\n",
       "        959., 969., 979., 989., 999.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noise_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctx_max_noise_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_35235/354781882.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  actions = one_hot_actions(torch.load(actions_path))\n"
     ]
    }
   ],
   "source": [
    "video_id = \"snippy-chartreuse-mastiff-f79998db196d-20220401-224517.chunk_001\"\n",
    "mp4_path = f\"sample_data/{video_id}.mp4\"\n",
    "actions_path = f\"sample_data/{video_id}.actions.pt\"\n",
    "video = (\n",
    "    read_video(mp4_path, pts_unit=\"sec\")[0].float() / 255\n",
    ")  # Normalize pixel values to [0, 1] range\n",
    "actions = one_hot_actions(torch.load(actions_path))\n",
    "offset = 100\n",
    "video = video[offset : offset + total_frames].unsqueeze(\n",
    "    0\n",
    ")  # Selects a specific segment using offset\n",
    "actions = actions[offset : offset + total_frames].unsqueeze(\n",
    "    0\n",
    ")  # Selects a specific segment using offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 360, 640, 3])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 25])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 360, 640, 3])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_prompt_frames = 8\n",
    "x = video[:, :n_prompt_frames]\n",
    "x = x.to(device)\n",
    "actions = actions.to(device)\n",
    "\n",
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 3, 360, 640])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 576, 16])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaling_factor = 0.07843137255\n",
    "x = rearrange(x, \"b t h w c -> (b t) c h w\")\n",
    "print(x.size())\n",
    "H, W = x.shape[-2:]\n",
    "with torch.no_grad():\n",
    "    x = vae.encode(x * 2 - 1).mean * scaling_factor\n",
    "\n",
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 16, 18, 32])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = rearrange(\n",
    "    x,\n",
    "    \"(b t) (h w) c -> b t c h w\",\n",
    "    t=n_prompt_frames,\n",
    "    h=H // vae.patch_size,\n",
    "    w=W // vae.patch_size,\n",
    ")\n",
    "\n",
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000])\n",
      "torch.Size([1000])\n",
      "torch.Size([1000])\n",
      "torch.Size([1000, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "betas = sigmoid_beta_schedule(max_noise_level).to(device) \n",
    "print(betas.size())\n",
    "alphas = 1.0 - betas\n",
    "print(alphas.size())\n",
    "alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
    "print(alphas_cumprod.size())\n",
    "alphas_cumprod = rearrange(alphas_cumprod, \"T -> T 1 1 1\")\n",
    "print(alphas_cumprod.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = n_prompt_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 16, 18, 32])\n",
      "torch.Size([1, 12, 16, 18, 32])\n",
      "torch.Size([1, 13, 16, 18, 32])\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "chunk = torch.randn((B, 1, *x.shape[-3:]), device=device)\n",
    "chunk = torch.clamp(chunk, -noise_abs_max, +noise_abs_max)\n",
    "print(chunk.size())\n",
    "print(x.size())\n",
    "x = torch.cat([x, chunk], dim=1)\n",
    "print(x.size())\n",
    "start_frame = max(0, i + 1 - model.max_frames)\n",
    "print(start_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min(100, 30) = 30\n",
      "ctx_noise_idx 30\n",
      "t_ctx torch.Size([1, 8]) tensor([[299, 299, 299, 299, 299, 299, 299, 299]], device='cuda:0')\n",
      "t torch.Size([1, 1]) tensor([[999]], device='cuda:0')\n",
      "torch.Size([1, 1]) tensor([[989]], device='cuda:0')\n",
      "torch.Size([1, 9]) tensor([[299, 299, 299, 299, 299, 299, 299, 299, 999]], device='cuda:0')\n",
      "torch.Size([1, 9]) tensor([[299, 299, 299, 299, 299, 299, 299, 299, 989]], device='cuda:0')\n",
      "torch.Size([1, 13, 16, 18, 32])\n",
      "torch.Size([1, 9])\n",
      "torch.Size([1, 9])\n",
      "t_next tensor([[299, 299, 299, 299, 299, 299, 299, 299, 989]], device='cuda:0')\n",
      "torch.Size([1, 12, 16, 18, 32])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (8) must match the size of tensor b (12) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 39\u001b[0m\n\u001b[1;32m     37\u001b[0m ctx_noise \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mclamp(ctx_noise, \u001b[38;5;241m-\u001b[39mnoise_abs_max, \u001b[38;5;241m+\u001b[39mnoise_abs_max)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(ctx_noise\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m---> 39\u001b[0m noise \u001b[38;5;241m=\u001b[39m \u001b[43malphas_cumprod\u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx_curr\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m alphas_cumprod[t[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]])\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m*\u001b[39m ctx_noise\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(noise\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m     41\u001b[0m x_curr[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m noise\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (8) must match the size of tensor b (12) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "for noise_idx in reversed(range(1, ddim_noise_steps + 1)):\n",
    "    ctx_noise_idx = min(noise_idx, ctx_max_noise_idx)\n",
    "    print(f\"min({noise_idx}, {ctx_max_noise_idx}) = {ctx_noise_idx}\")\n",
    "    print(\"ctx_noise_idx\", ctx_noise_idx)\n",
    "    \n",
    "    t_ctx = torch.full(\n",
    "                (B, i), noise_range[ctx_noise_idx], dtype=torch.long, device=device\n",
    "            )\n",
    "    print(\"t_ctx\", t_ctx.size(),t_ctx)\n",
    "    t = torch.full(\n",
    "                (B, 1), noise_range[noise_idx], dtype=torch.long, device=device\n",
    "            )\n",
    "    print(\"t\", t.size(),t)\n",
    "    t_next = torch.full(\n",
    "                (B, 1), noise_range[noise_idx - 1], dtype=torch.long, device=device\n",
    "            )\n",
    "    print(t_next.size(),t_next)\n",
    "    \n",
    "    t_next = torch.where(t_next < 0, t, t_next)\n",
    "    t = torch.cat([t_ctx, t], dim=1)\n",
    "    t_next = torch.cat([t_ctx, t_next], dim=1)\n",
    "    print(t.size(),t)\n",
    "    print(t_next.size(),t_next)\n",
    "\n",
    "    # sliding window\n",
    "    x_curr = x.clone()\n",
    "    x_curr = x_curr[:, start_frame:]\n",
    "    t = t[:, start_frame:]\n",
    "    t_next = t_next[:, start_frame:]\n",
    "    print(x_curr.size())\n",
    "    print(t.size())\n",
    "    print(t_next.size())\n",
    "    print(\"t_next\", t_next)\n",
    "\n",
    "    # add some noise to the context\n",
    "    ctx_noise = torch.randn_like(x_curr[:, :-1])\n",
    "    ctx_noise = torch.clamp(ctx_noise, -noise_abs_max, +noise_abs_max)\n",
    "    print(ctx_noise.size())\n",
    "    noise = alphas_cumprod[t[:, :-1]].sqrt() * x_curr[:, :-1] + (1 - alphas_cumprod[t[:, :-1]]).sqrt() * ctx_noise\n",
    "    print(noise.size())\n",
    "    x_curr[:, :-1] = noise\n",
    "\n",
    "    # Use DiT model to predict and remove noise\n",
    "    with torch.no_grad():\n",
    "        with autocast(\"cuda\", dtype=torch.half):\n",
    "            v = model(x_curr, t, actions[:, start_frame : i + 1])\n",
    "    print(v.size())\n",
    "\n",
    "    x_start = (\n",
    "                alphas_cumprod[t].sqrt() * x_curr - (1 - alphas_cumprod[t]).sqrt() * v\n",
    "            )\n",
    "    x_noise = ((1 / alphas_cumprod[t]).sqrt() * x_curr - x_start) / (\n",
    "        1 / alphas_cumprod[t] - 1\n",
    "    ).sqrt()\n",
    "\n",
    "    # get frame prediction\n",
    "    x_pred = (\n",
    "        alphas_cumprod[t_next].sqrt() * x_start\n",
    "        + x_noise * (1 - alphas_cumprod[t_next]).sqrt()\n",
    "    )\n",
    "    # Replace the generated frame in the input tensor\n",
    "    print(x.size())\n",
    "    x[:, -1:] = x_pred[:, -1:]\n",
    "    print(x.size())\n",
    "    print()\n",
    "    print()\n",
    "\n",
    "    \n",
    "    if noise_idx == 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oasis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
